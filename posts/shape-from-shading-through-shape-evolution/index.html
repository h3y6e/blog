<!DOCTYPE html> <html lang=ja > <meta charset=utf-8  /> <meta name=viewport  content="width=device-width,initial-scale=1,maximum-scale=1" /> <meta name=author  content=heyhoe  /> <meta name=theme-color  content="#2f2f2f" /> <link rel=stylesheet  href="/css/a5ebec.css" /> <link rel=icon  href="/assets/favicon/favicon.ico" /> <link rel=apple-touch-icon  href="/assets/favicon/apple-touch-icon.png" /> <meta property="og:site_name" content="#a5ebec" /> <meta property="og:image" content="https://blog.h3y6e.com/assets/a5ebec.jpg" /> <meta name="twitter:card" content=summary  /> <meta name="twitter:site" content="@5ebec" /> <meta name="twitter:creator" content="@5ebec" /> <title> [論文読み] Shape from Shading through Shape Evolution :: #a5ebec </title> <meta name=description  content="論文読み : 形状進化による3Dデータを用いたShape-from-Shading" /> <meta property="og:type" content=article  /> <meta property="og:title" content="[論文読み] Shape from Shading through Shape Evolution :: #a5ebec" /> <meta property="og:url" content="https://blog.h3y6e.com/posts/shape-from-shading-through-shape-evolution/index.html" /> <meta property="og:description" content="論文読み : 形状進化による3Dデータを用いたShape-from-Shading" /> <header> <div class=header-inner > <div class=header-logo > <a href="/"><div class=logo >#a5ebec</div></a> </div> <label for=trigger  class=menu >menu</label> </div> <input type=checkbox  id=trigger  /> <nav> <ul> <li><a href="/">Posts</a> <li><a href="/tags">Tags</a> <li><a href="/about">About</a> </ul> </nav> </header> <div class=franklin-headline > <h1 class=title >[論文読み] Shape from Shading through Shape Evolution</h1> <div class=date >2019-08-29</div><span class=tags ><a href="/tags/paper">#paper</a> <a href="/tags/cv">#cv</a> </span></div><div class=franklin-toc ><ol><li><a href="#what_it_is_about">What it is about</a><li><a href="#why_it_is_worthy_researching">Why it is worthy researching</a><li><a href="#key_idea">Key idea</a><li><a href="#how_it_is_validated_experimental_setup_and_results">How it is validated （experimental setup and results）</a><li><a href="#limitations">Limitations</a><li><a href="#what_you_thought">What you thought</a><li><a href="#papers_to_read_before_and_after_the_work">Papers to read before and after the work</a></ol></div> <div class=franklin-content > <ul> <li><p>Shape from Shading through Shape Evolution</p> <li><p>Dawei Yang, Jia Deng</p> <li><p>CVPR, 2018</p> <li><p><a href="https://arxiv.org/pdf/1712.02961.pdf">arXiv</a>, <a href="https://www.semanticscholar.org/paper/Shape-from-Shading-Through-Shape-Evolution-Yang-Deng/d74a576cc311841c3ff8070262e928c090e41f59">SemanticScholar</a></p> </ul> <h2 id=what_it_is_about ><a href="#what_it_is_about" class=header-anchor >What it is about</a></h2> <p>実画像のShape-from-ShadingをDNNに学習させる際のデータとして、単純なプリミティブ（珠、立方体、等）を用いて作成された3Dデータを用いる手法を提案。</p> <h2 id=why_it_is_worthy_researching ><a href="#why_it_is_worthy_researching" class=header-anchor >Why it is worthy researching</a></h2> <p>既存手法では全て人手で作成されたデータを用いていた。</p> <p>提案手法ではシンプルなプリミティブを組み合わせて複雑な形状のデータセットを適宜作成して、 DNNの学習をすることでデータ不足を解決する。 トレーニングに外部データセットを用いることなく、実画像に対するShape-from-ShadingにおいてState-of-the-Art（SoTA）を達成。</p> <h2 id=key_idea ><a href="#key_idea" class=header-anchor >Key idea</a></h2> <h4 id=shape_representation ><a href="#shape_representation" class=header-anchor >Shape Representation</a></h4> <p>初期形状は球、円柱、立方体、円錐の4つの形状で構成されており、それらは以下の函数で表すことが出来る。 <img src="https://user-images.githubusercontent.com/38322494/63978227-da9a0700-caf0-11e9-91b4-7af7e8d0c61d.png" alt="Screenshot from 2019-08-30 06-38-36" /> Computation graphで表現すると以下のようになる。 <img src="https://user-images.githubusercontent.com/38322494/63973257-5db56000-cae5-11e9-839a-020fa8a9a7b4.png" alt="Screenshot from 2019-08-30 00-57-03" /></p> <p>形状変換（平行移動、回転、拡大縮小） <img src="https://user-images.githubusercontent.com/38322494/63973265-60b05080-cae5-11e9-941b-1d9bf10b0e90.png" alt="Screenshot from 2019-08-30 00-57-16" /></p> <p>形状結合 <img src="https://user-images.githubusercontent.com/38322494/63973273-627a1400-cae5-11e9-9bad-53d556c466d9.png" alt="Screenshot from 2019-08-30 00-57-23" /></p> <h4 id="進化アルゴリズム"><a href="#進化アルゴリズム" class=header-anchor >進化アルゴリズム</a></h4> <p>形状変換と形状結合を繰り返すことでより複雑な形状へ進化させる。 Computation graphが大きくなりすぎないように（制約がなければ平均計算コストは指数関数的に増加する）、計算回数が線形になるようにグラフの成長を制限する。また、形状結合前後で変化がほぼ無いケースを検出し排除する等、進化が遅くならないようにする。</p> <p>バリデーションを実画像で行うため、実画像が持つ形状とかけ離れた形状を持つトレーニングデータは捨てられる。 <img src="https://user-images.githubusercontent.com/38322494/63980943-0ff62300-caf8-11e9-8830-8dd2a6c71bf9.png" alt="Screenshot from 2019-08-30 07-30-14" /></p> <p>shape-from-shadingネットワークは <a href="https://arxiv.org/pdf/1603.06937.pdf">Stacked Hourglass Network</a> を使用している。 <img src="https://user-images.githubusercontent.com/38322494/63980458-af1a1b00-caf6-11e9-9f67-906a8270f5e7.png" alt="Screenshot from 2019-08-30 07-20-29" /></p> <h2 id=how_it_is_validated_experimental_setup_and_results ><a href="#how_it_is_validated_experimental_setup_and_results" class=header-anchor >How it is validated （experimental setup and results）</a></h2> <p><a href="http://www.cs.toronto.edu/~rgrosse/intrinsic/gallery.html">MIT-Berkeley Intrinsic Image</a> データセットを用いて SIRFS &#40;<a href="https://www2.eecs.berkeley.edu/Pubs/TechRpts/2013/EECS-2013-117.pdf">Shape, Illumination, and Reflectance from Shading</a>&#41; と比較。 提案手法は形状のみを進化させテクスチャは進化させないので、テクスチャレス画像を使用したSIRFSと比較する。 <img src="https://user-images.githubusercontent.com/38322494/63975239-863f5900-cae9-11e9-82e3-98c87b0b5e88.png" alt="Screenshot from 2019-08-30 00-57-52" /></p> <p>他の手法（ランダム、 SIRFS, <a href="https://arxiv.org/pdf/1512.03012.pdf">ShapeNet</a>）との比較。 <img src="https://user-images.githubusercontent.com/38322494/63981084-a1fe2b80-caf8-11e9-8193-f9febb407a71.png" alt="Screenshot from 2019-08-30 07-33-59" /></p> <h2 id=limitations ><a href="#limitations" class=header-anchor >Limitations</a></h2> <p>結局、トレーニングでも表面法線のground truthを含むデータセットは必要？</p> <h2 id=what_you_thought ><a href="#what_you_thought" class=header-anchor >What you thought</a></h2> <p>プリミティブから3Dデータを構築するごとにDNNがfine-tuningされていくため、最適な結果を見つけるためにはいくつかの重みで検証する必要がある。</p> <h2 id=papers_to_read_before_and_after_the_work ><a href="#papers_to_read_before_and_after_the_work" class=header-anchor >Papers to read before and after the work</a></h2> <p><a href="https://arxiv.org/pdf/1603.06937.pdf">Stacked Hourglass Networks for Human Pose Estimation</a></p> <p><a href="https://www.semanticscholar.org/paper/Realistic-Adversarial-Examples-in-3D-Meshes-Yang-Xiao/047670f1b38e8df8f5cb6d623e939eecbc2d2315">Realistic Adversarial Examples in 3D Meshes</a> </p> <p><a href="https://www.semanticscholar.org/paper/MeshAdv&#37;3A-Adversarial-Meshes-for-Visual-Recognition-Xiao-Yang/1a83564d61aebde360c0be4834cf6eb4c472c1bd">MeshAdv: Adversarial Meshes for Visual Recognition</a> </p> <p><a href="https://www.semanticscholar.org/paper/Learning-to-Generate-3-D-Training-Data-through-Yang/d8bf8a6bcee94ac70a95934cafa858051d74c05e">Learning to Generate 3 D Training Data through Hybrid Gradient</a></p> <footer class=page-foot > <div class=copyright > <span> &copy; 2019-2022 <a href="https://h3y6e.com">heyhoe</a> </span> <span>::Powered py <a href="https://franklinjl.org/">Franklin.jl</a></span> </div> </footer> </div>
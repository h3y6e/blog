<!DOCTYPE html> <html lang=ja > <meta charset=utf-8  /> <meta name=viewport  content="width=device-width,initial-scale=1,maximum-scale=1" /> <meta name=author  content=5ebec  /> <meta name=description  content="へいほぅの殴り書き" /> <meta name=theme-color  content="#a5ebec" /> <link rel=stylesheet  href="/css/a5ebec.css" /> <link rel=icon  href="/assets/favicon/favicon.ico" /> <link rel=apple-touch-icon  href="/assets/favicon/apple-touch-icon.png" /> <title>[論文読み] Shape from Shading through Shape Evolution</title> <header> <div class=header-inner > <div class=header-logo > <a href="/"><div class=logo >#a5ebec</div></a> </div> <label for=trigger  class=menu >menu</label> </div> <input type=checkbox  id=trigger  /> <nav> <ul> <li><a href="/">Posts</a> <li><a href="/tags">Tags</a> <li><a href="/about">About</a> </ul> </nav> </header> <div class=franklin-headline > <h1 class=title >[論文読み] Shape from Shading through Shape Evolution</h1> <div class=date >2019-08-29</div> <span class=tags > <a href="/tags/paper">#paper</a> <a href="/tags/cv">#cv</a> </span></div><div class=franklin-toc ><ol><li><a href="#what_it_is_about">What it is about</a><li><a href="#why_it_is_worthy_researching">Why it is worthy researching</a><li><a href="#key_idea">Key idea</a><li><a href="#how_it_is_validated_experimental_setup_and_results">How it is validated &#40;experimental setup and results&#41;</a><li><a href="#limitations">Limitations</a><li><a href="#what_you_thought">What you thought</a><li><a href="#papers_to_read_before_and_after_the_work">Papers to read before and after the work</a></ol></div><div class=franklin-content ><ul> <li><p>Shape from Shading through Shape Evolution</p> <li><p>Dawei Yang, Jia Deng</p> <li><p>CVPR, 2018</p> <li><p><a href="https://arxiv.org/pdf/1712.02961.pdf">arXiv</a>, <a href="https://www.semanticscholar.org/paper/Shape-from-Shading-Through-Shape-Evolution-Yang-Deng/d74a576cc311841c3ff8070262e928c090e41f59">SemanticScholar</a></p> </ul> <h2 id=what_it_is_about ><a href="#what_it_is_about">What it is about</a></h2> <p>実画像の Shape-from-Shading を DNN に学習させる際のデータとして，単純なプリミティブ &#40;球，立方体，等&#41; を用いて作成された 3D データを用いる手法を提案．</p> <h2 id=why_it_is_worthy_researching ><a href="#why_it_is_worthy_researching">Why it is worthy researching</a></h2> <p>既存手法では全て人手で作成されたデータを用いていた．</p> <p>提案手法ではシンプルなプリミティブを組み合わせて複雑な形状のデータセットを適宜作成して， DNN の学習を行うことでデータ不足を解決する． トレーニングに外部データセットを用いることなく，実画像に対する Shape-from-Shading において State-of-the-Art &#40;SoTA&#41; を達成．</p> <h2 id=key_idea ><a href="#key_idea">Key idea</a></h2> <h4 id=shape_representation ><a href="#shape_representation">Shape Representation</a></h4> <p>初期形状は球，円柱，立方体，円錐の４つの形状で構成されており，それらは以下の函数で表すことができる． <img src="https://user-images.githubusercontent.com/38322494/63978227-da9a0700-caf0-11e9-91b4-7af7e8d0c61d.png" alt="Screenshot from 2019-08-30 06-38-36" /> Computation graph で表現すると以下のようになる． <img src="https://user-images.githubusercontent.com/38322494/63973257-5db56000-cae5-11e9-839a-020fa8a9a7b4.png" alt="Screenshot from 2019-08-30 00-57-03" /></p> <p>形状変換&#40;平行移動，回転，拡大縮小&#41; <img src="https://user-images.githubusercontent.com/38322494/63973265-60b05080-cae5-11e9-941b-1d9bf10b0e90.png" alt="Screenshot from 2019-08-30 00-57-16" /></p> <p>形状結合 <img src="https://user-images.githubusercontent.com/38322494/63973273-627a1400-cae5-11e9-9bad-53d556c466d9.png" alt="Screenshot from 2019-08-30 00-57-23" /></p> <h4 id="進化アルゴリズム"><a href="#進化アルゴリズム">進化アルゴリズム</a></h4> <p>形状変換と形状結合を繰り返すことでより複雑な形状へ進化させる． Computation graph が大きくなりすぎないように（制約がなければ平均計算コストは指数関数的に増加する），計算回数が線形になるようにグラフの成長を制限する．また，形状結合前後で変化がほぼ無いケースを検出し排除する等，進化が遅くならないようにする．</p> <p>バリデーションを実画像で行うため，実画像が持つ形状とかけ離れた形状を持つトレーニングデータは捨てられる． <img src="https://user-images.githubusercontent.com/38322494/63980943-0ff62300-caf8-11e9-8830-8dd2a6c71bf9.png" alt="Screenshot from 2019-08-30 07-30-14" /></p> <p>shape-from-shading ネットワークは <a href="https://arxiv.org/pdf/1603.06937.pdf">Stacked Hourglass Network</a> を使用している． <img src="https://user-images.githubusercontent.com/38322494/63980458-af1a1b00-caf6-11e9-9f67-906a8270f5e7.png" alt="Screenshot from 2019-08-30 07-20-29" /></p> <h2 id=how_it_is_validated_experimental_setup_and_results ><a href="#how_it_is_validated_experimental_setup_and_results">How it is validated &#40;experimental setup and results&#41;</a></h2> <p><a href="http://www.cs.toronto.edu/~rgrosse/intrinsic/gallery.html">MIT-Berkeley Intrinsic Image</a> データセットを用いて SIRFS &#40;<a href="https://www2.eecs.berkeley.edu/Pubs/TechRpts/2013/EECS-2013-117.pdf">Shape, Illumination, and Reflectance from Shading</a>&#41; と比較 提案手法は形状のみを進化させテクスチャは進化させないので，テクスチャレス画像を使用した SIRFS と比較する <img src="https://user-images.githubusercontent.com/38322494/63975239-863f5900-cae9-11e9-82e3-98c87b0b5e88.png" alt="Screenshot from 2019-08-30 00-57-52" /></p> <p>他の手法&#40;ランダム, SIRFS, <a href="https://arxiv.org/pdf/1512.03012.pdf">ShapeNet</a>&#41; との比較 <img src="https://user-images.githubusercontent.com/38322494/63981084-a1fe2b80-caf8-11e9-8193-f9febb407a71.png" alt="Screenshot from 2019-08-30 07-33-59" /></p> <h2 id=limitations ><a href="#limitations">Limitations</a></h2> <p>結局，トレーニングでも表面法線の ground truth を含むデータセットは必要?</p> <h2 id=what_you_thought ><a href="#what_you_thought">What you thought</a></h2> <p>プリミティブから 3D データを構築するごとに DNN が fine-tuning されていくため，最適な結果を見つけるためにはいくつかの重みで検証する必要がある．</p> <h2 id=papers_to_read_before_and_after_the_work ><a href="#papers_to_read_before_and_after_the_work">Papers to read before and after the work</a></h2> <p><a href="https://arxiv.org/pdf/1603.06937.pdf">Stacked Hourglass Networks for Human Pose Estimation</a></p> <p><a href="https://www.semanticscholar.org/paper/Realistic-Adversarial-Examples-in-3D-Meshes-Yang-Xiao/047670f1b38e8df8f5cb6d623e939eecbc2d2315">Realistic Adversarial Examples in 3D Meshes</a> </p> <p><a href="https://www.semanticscholar.org/paper/MeshAdv&#37;3A-Adversarial-Meshes-for-Visual-Recognition-Xiao-Yang/1a83564d61aebde360c0be4834cf6eb4c472c1bd">MeshAdv: Adversarial Meshes for Visual Recognition</a> </p> <p><a href="https://www.semanticscholar.org/paper/Learning-to-Generate-3-D-Training-Data-through-Yang/d8bf8a6bcee94ac70a95934cafa858051d74c05e">Learning to Generate 3 D Training Data through Hybrid Gradient</a></p> <footer class=page-foot > <div class=copyright > <span>&copy; 2020 <a href="https://a5e.be/c">5ebec</a></span> <span>::Powered py <a href="https://franklinjl.org/">Franklin.jl</a></span> </div> </footer> </div>